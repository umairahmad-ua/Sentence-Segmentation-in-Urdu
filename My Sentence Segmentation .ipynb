{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umair Ahmad\n",
    "# 21i-2081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzvXrOhqgWgK"
   },
   "source": [
    "#  Sentence Segmentation (My Sentance Tokenizer) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sygH6zpg8tH"
   },
   "source": [
    "Import Libraries \n",
    "\n",
    "*   Codecs for file handling\n",
    "*   Pandas for file dataframes \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uJXRLlj-UBqP"
   },
   "outputs": [],
   "source": [
    "#Umair Ahmad\n",
    "# 21i-2081\n",
    "import codecs\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYxX2mFvSI04"
   },
   "source": [
    "# My_sentence_tokenizer() \n",
    "\n",
    "\n",
    "*   Function receive the text in string and return the segmented sentences which are also be called sentence tokens \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Urdu_conjunctions[ ] \n",
    "\n",
    "* Contains the possible words to join the urdu sentences, however we will use it to find the concordance\n",
    "\n",
    "# Urdu_sentance_finishers[]\n",
    "\n",
    "* Contains the possible words to end the urdu sentences, we will use it to find the segments of sentences\n",
    "\n",
    "# Output.csv\n",
    "\n",
    "* This function will also generate the output csv file in the local directory that contains the all segmented sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vV7Ru_oP_SFk"
   },
   "outputs": [],
   "source": [
    "def my_sentence_tokenizer(words):\n",
    "\n",
    "  urdu_conjunctions=[\"کہ\",\"پر\",\"اور\",\"یا\",\"پہلے\",\"لیکن\",\"کیونکہ\",\"اگر\",\"جب تک\",\"جب\",\"اسی لیے\",\"ورنہ\",\"یاکہ\",\"یا\",\"نہ\",\"چونکہ\",\"اگرچہ\",\"پھر\",\"جونہی\",\"نہ\",\"صرف\",\"بلکہ\",\"تاکہ\",\"تو\",\"جتنا\",\"اتنا\",\"ہم\",\"تا\",\"جو\",\"یہی\",\"جسے\",\"کر\",\"چنانچہ\",\"مگر\",\"جوکہ\",\"کے\",\"جن\",\"کیا\",\"ہی\"]\n",
    "  urdu_sentance_finishers=[\"ہے\",\"ہیں\",\"ہو\",\"ہوں\",\"تھیں\",\"تھے\",\"تھی\",\"تھا\",\"گا\",\"گے\",\"گی\",\"سکا\",\"سکی\",\"ہوگا\",\"ہوگی\",\"گیا\",\"گئیں\",\"کیں\",\"\"]\n",
    "\n",
    "\n",
    "  words = str(words).split(\" \")\n",
    "  for x in list(words): \n",
    "      if x==' ':\n",
    "          words.remove(x)\n",
    "\n",
    "\n",
    "  for index, x in enumerate(words):\n",
    "    if index+1 < len(words):\n",
    "      if '؟'  in str(x) or '!'  in str(x):\n",
    "        words.insert(index+1,\"۔\")\n",
    "      if x in urdu_sentance_finishers:\n",
    "        if words[index+1] not in urdu_conjunctions:\n",
    "          words.insert(index+1,\"۔\")\n",
    "\n",
    "\n",
    "    \n",
    "  data=\" \".join(words)\n",
    "  data_lst=data.split(\"۔\")\n",
    "  temp_str=\"\"\n",
    "  for index, x in enumerate(data_lst):\n",
    "    if len(x)<50 and index != 0:\n",
    "      temp=data_lst[index-1]+x\n",
    "      data_lst[index-1]=temp\n",
    "      data_lst.pop(index)\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "  for index, x in enumerate(data_lst):\n",
    "    if \"  \" in x:\n",
    "      temp=x.replace(\"  \", \"\")\n",
    "      data_lst[index]=temp\n",
    "  dict = {'Sentences': data_lst}   \n",
    "  df = pd.DataFrame(dict)  \n",
    "  df.to_csv(\"output.csv\")\n",
    "  return data_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MsqmfZzfRfOv",
    "outputId": "eb63f3d5-4e58-4339-bf85-9550d52a4c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے', ' کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا', ' میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا', ' ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو\\n']\n"
     ]
    }
   ],
   "source": [
    "path=\"C:/Users/Umair/Downloads/\"\n",
    "filename=\"sent-test.txt\"\n",
    "data = codecs.open(path+filename, encoding='utf-8')\n",
    "data=data.read()\n",
    "sentences=my_sentence_tokenizer(data)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrnSbKeIUrO6"
   },
   "source": [
    "# Test_Suites()\n",
    "Return the dictionary of different test cases which are mention below if the test cases contain 1 it means test fails.\n",
    "#Test Cases\n",
    "\n",
    "* Check Segmentation Count\n",
    "* Check White Spaces\n",
    "* Check Question Mark\n",
    "* Check Exclamation Mark\n",
    "* Check English Words\n",
    "* Check Numeric Digits\n",
    "* Check relation with Conjunctions\n",
    "* Check If Break Small Sentences \n",
    "\n",
    "### As we can Clearly see in the results it passes all the mentioned test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwJyVwVFkcr4",
    "outputId": "8c47c57e-17d6-4de7-bfff-ec58ac071155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'check_segment_count': [], 'white_Spaces': [], 'check_question_mark': [], 'check_exclamation_mark': [], 'check_english_words': [], 'check_numaric_digits': [], 'check_conjunction_words': [], 'check_small_sentences': []}\n"
     ]
    }
   ],
   "source": [
    "def test_suite():\n",
    "  test_cases={'check_segment_count':[],\n",
    "          'white_Spaces':[],\n",
    "          'check_question_mark':[],\n",
    "          'check_exclamation_mark':[],\n",
    "          'check_english_words':[],\n",
    "          'check_numaric_digits':[],\n",
    "          'check_conjunction_words':[],\n",
    "          'check_small_sentences':[],\n",
    "          }\n",
    "  token=my_sentence_tokenizer(\"عقل خان کے مطابق اس خوبصورت چراگاہ کو کنڈیل شئی بانال کہا جاتا ہے کنڈیل شئی بانال کے اس خوبصورت میدان کو اگر سویٹزرلینڈ کے کسی ہرے بھرے میدانی علاقے سے تشبیہہ دی جائے تو کچھ غلط نہیں ہوگا میدان میں داخل ہوتے ہی کچھ دیر آرام کرنے کی میری خواہش پر سب نے لبیک کہا ایسا لگا جیسے ان کی دل کی بات میرے لبوں سے ادا ہوئی ہو\")# contain three different sentenses\n",
    "  if len(token)!=3:\n",
    "    test_cases[\"check_segment_count\"].append(\"fail\")\n",
    "  token=my_sentence_tokenizer(\"وہ جن کے دَور میں لوگوں کو پیٹ بھر کے کھانا نصیب  ہوتا تھا\")# sentence contains extra white spaces\n",
    "  if \"  \" in token:\n",
    "    test_cases[\"white_Spaces\"].append(\"fail\")\n",
    "\n",
    "\n",
    "  token=my_sentence_tokenizer(\"آج سے ہم بھائی بھائی بن گئے ہیں، تو کیوں نہ اسی رسم کا اعادہ کیا جائے؟‘‘ محمد شاہ کے پاس سر جھکانے کے علاوہ کوئی چارہ نہیں تھا\")# sentence contains question mark\n",
    "  if(len(token)!=2):\n",
    "      test_cases[\"check_question_mark\"].append(\"fail\")\n",
    "\n",
    "      \n",
    "  token=my_sentence_tokenizer(\"آج سے ہم بھائی بھائی بن گئے ہیں، تو کیوں نہ اسی رسم کا اعادہ کیا جائے!‘‘ محمد شاہ کے پاس سر جھکانے کے علاوہ کوئی چارہ نہیں تھا\")# sentence contains exclamation mark\n",
    "  if(len(token)!=2):\n",
    "      test_cases[\"check_exclamation_mark\"].append(\"fail\")\n",
    "        \n",
    "\n",
    "  token=my_sentence_tokenizer (\"Hello i am ئمایر\")# sentence contains english words \n",
    "  if len(token)>1:\n",
    "      test_cases[\"check_english_words\"].append(\"fail\")\n",
    "        \n",
    "\n",
    "  token=my_sentence_tokenizer(\"نوجوانوں کوبعد ازاں فائرنگ کرکے شہید کردیاجاتا ہے کے ایم ایس کی رپورٹ کے مطابق مقبوضہ کشمیر میں بھارتی مظالم کے خلاف کشمیریوں کی مزاحمت بڑھتی جارہی ہے اور وہ پوسٹروں کے ذریعے بھارت مخالف مظاہرے جاری رکھنے پر پْر عزم رہتے ہوئے بھارتی فورسز کے سامنے سینہ سپر ہیں سری نگر اور اس کے نواحی علاقوں میں لگائے گئے پوسٹرز میں کشمیریوں کی جانب سے بھارت کے غیرقانونی اقدامات کے خلاف سول نافرمانی کی اپیل بھی کی گئی ہے اور اس تحریک میں بہت سے کشمیری نوجوان شامل ہورہے ہیں ادھر بھارتی وزارت داخلہ نے کشمیر میں فوج، بحریہ اورفضائیہ کے خصوصی اہلکار بھی تعینات کردیئے ہیں تاکہ کسی بھی ممکنہ سخت احتجاج سے نبٹا جا سکے کشمیر میں جس طرح کے مظالم ڈھائے جارہے ہیں اس پر کشمیر کمیٹی کے چیئرمین نے کہا کہ مقبوضہ کشمیر میں بھارتی لاک ڈاوَن 4354 کی مثال دنیا بھر میں نہیں ملتی ان کا کہنا تھا (UN) \")# sentence contains Numarics\n",
    "  if \"4354\" not in token[-1]:\n",
    "      test_cases[\"check_numaric_digits\"].append(\"fail\")\n",
    "      \n",
    "  \n",
    "  token=my_sentence_tokenizer(\" یہی اصول تاریخ اور انسانی معاشرت پر بھی لاگو ہوتا ہے کہ جس چیز کو جتنی سختی سے دبایا جائے، وہ اتنی ہی قوت سے ابھر کر سامنے آتی ہے چنانچہ اورنگزیب کے بعد بھی یہی کچھ ہوا اور محمد شاہ کے دور میں وہ تمام فنون پوری آب و تاب سے سامنے آ گئے\")# sentence contains conjunction words which can not be tokenized \n",
    "  if(len(token)!=1):\n",
    "      test_cases[\"check_conjunction_words\"].append(\"fail\")\n",
    "      \n",
    "  token=my_sentence_tokenizer(\"انھوں نے کہا: ‘‘آپ نے موسیقی قتل کر دی ہے اسے دفنانے جا رہے ہیں\")# its a one sentense \n",
    "  if(len(token)!=1):\n",
    "      test_cases[\"check_small_sentences\"].append(\"fail\")\n",
    "\n",
    "  return test_cases\n",
    "\n",
    "result=test_suite()\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KeV2Z6rQAti"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzQ3Ce4bWyLc"
   },
   "source": [
    "# Comparison of My Sentence Tokenizer with Urdu Hack\n",
    "* As we can clearly see with two example’s results that Urdu hack ignore the conjunction words which cannot be tokenized but Urdu hack forcefully split it. However, my Sentence Tokenizer can handle these cases and also produce the same results as Urdu Hack’s Sentence Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3di841NYBua",
    "outputId": "f6e86fbf-ed50-4d43-c101-8c0b7f4599c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urduhack\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/04/3393a9626b766cfee3187e9ccfa27e73061c24646d60be22a0652de95b4f/urduhack-1.1.1-py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 4.0MB/s \n",
      "\u001b[?25hCollecting tensorflow-datasets~=3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/c9/d97bdf931edbae9aebc767633d088bd674136d5fe7587ef693b7cb6a1883/tensorflow_datasets-3.2.1-py3-none-any.whl (3.4MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4MB 5.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: Click~=7.1 in /usr/local/lib/python3.7/dist-packages (from urduhack) (7.1.2)\n",
      "Collecting tf2crf\n",
      "  Downloading https://files.pythonhosted.org/packages/53/17/4a6901f88d763846ea13a46f0d0228104a0b5b75fd45f5354186630e377e/tf2crf-0.1.31-py2.py3-none-any.whl\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from urduhack) (2019.12.20)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.23.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (3.12.4)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.28.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (4.41.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.1.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.15.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.12.1)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (20.3.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.3.3)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\n",
      "Collecting tensorflow-addons>=0.8.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
      "\u001b[K     |████████████████████████████████| 706kB 43.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf2crf->urduhack) (2.4.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (1.24.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets~=3.1->urduhack) (54.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack) (1.53.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.8.2->tf2crf->urduhack) (2.7.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.4.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.3.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.36.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.32.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.10.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.4.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.3.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (1.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (1.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (3.7.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf->urduhack) (3.4.1)\n",
      "Installing collected packages: tensorflow-datasets, tensorflow-addons, tf2crf, urduhack\n",
      "  Found existing installation: tensorflow-datasets 4.0.1\n",
      "    Uninstalling tensorflow-datasets-4.0.1:\n",
      "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
      "Successfully installed tensorflow-addons-0.12.1 tensorflow-datasets-3.2.1 tf2crf-0.1.31 urduhack-1.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install urduhack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byhu3L55sdno",
    "outputId": "7f70025c-68ad-453c-e868-f0f62f29ab19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import urduhack\n",
    "from urduhack.tokenization import sentence_tokenizer\n",
    "import codecs\n",
    "one_sentence = \"انھوں نے کہا: ‘‘آپ نے موسیقی قتل کر دی ہے اسے دفنانے جا رہے ہیں\" # its a one sentense\n",
    "\n",
    "urdu_hack_sentences = sentence_tokenizer(one_sentence)\n",
    "mysentence_tokenizerken=my_sentence_tokenizer(one_sentence)\n",
    "print (len(urdu_hack_sentences))\n",
    "print (len(mysentence_tokenizerken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2F4xXDiv7cS5",
    "outputId": "d783c5e7-608b-447e-878a-1393529e5f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#one more Comparison\n",
    "one_sentence = \" یہی اصول تاریخ اور انسانی معاشرت پر بھی لاگو ہوتا ہے کہ جس چیز کو جتنی سختی سے دبایا جائے، وہ اتنی ہی قوت سے ابھر کر سامنے آتی ہے چنانچہ اورنگزیب کے بعد بھی یہی کچھ ہوا اور محمد شاہ کے دور میں وہ تمام فنون پوری آب و تاب سے سامنے آ گئے\" # sentence contains conjunction words which can not be tokenized \n",
    "\n",
    "\n",
    "urdu_hack_sentences = sentence_tokenizer(one_sentence)\n",
    "mysentence_tokenizerken=my_sentence_tokenizer(one_sentence)\n",
    "print (len(urdu_hack_sentences))\n",
    "print (len(mysentence_tokenizerken))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment 1 Sentence Segmentation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
